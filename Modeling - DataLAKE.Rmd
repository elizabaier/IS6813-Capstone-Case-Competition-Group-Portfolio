---
title: "Modeling"
author: "DataLAKE"
date: "`r Sys.Date()`"
format: 
  html:
    toc: true
    toc-depth: 3
    toc-location: left
    toc-float: true
    toc-title: "Contents"
    self-contained: true
execute:
  include: true
  eval: true
  warning: false
  message: false
---

## Introduction 

The goal of this modeling notebook is to examine MyCoke360 digital ordering behavior and identify patterns that drive cart abandonment. The target outcome for this analysis is whether a cart is abandoned within an order window. During our exploratory data analysis, we created a target variable and calculated an abandonment rate that was focused at the customer level. After shifting to the purchase window as our unit of measure, which is represented by the `CART_ID` above, we decided we needed to recalculate our target variable. Using the `EVENT_NAME` column, we created a target variable by comparing it to the `EVENT_TIMESTAMP_ADJ` column from the orders table. This timestamp has been adjusted to the local time zone and indicates when an order was created. We compared it to the occurrence of any of the following events in the `EVENT_NAME` column: `add_to_cart`,`update_cart`, or `UpdateCart_Cart_Clicked`. Previously, we used just the `add_to_cart` event, but given that the `google_analytics` data is known to have missed some events, we broadened it to include these other events. If `EVENT_TIMESTAMP_ADJ` occurred between the last of these events and the `CUTOFF_TIMESTAMP`, the cart was classified as not abandoned. If these events never appeared within that time window, the cart was also considered not abandoned. However, if those events occurred but no `EVENT_TIMESTAMP_ADJ` fell within the purchase window before the cutoff, the cart was classified as abandoned. This translates to a supervised learning binary classification problem, where the negative class (0) represents carts that are completed within the order window, and the positive class (1) represents carts that are abandoned.

This modeling report will cover:

  -  Feature engineering including window id, cart id, our updated target variable, total items, and days since last purchased. 
  -  Analysis of items in abandoned carts to answer the question - which items appear most often in abandoned carts?
  -  Events and abandoned carts analysis that shows our understanding of the events that lead to cart abandonment.
  -  Detailed information about our modeling efforts including an interrupted time series model, association rule mining, and two logistic regression models
  -  Identification of limitations of our modeling efforts, future continuation ideas, and a summary of our findings and conclusions. 

The insights gained from this analysis will helps us identify the behavioral patterns leading to cart abandonment. This will help us to develop interventions to prevent cart abandonment leading to increased customer engagement, increased revenue, and improved return on investment from Swire Coca-Cola's new MyCoke 360 interface. 

By the end of this modeling notebook, we expect to:

  -  Identify the most relevant features and event sequences that contribute to cart abandonment and recovery.
  -  Generate descriptive statistics and visualizations to highlight abandonment patterns by cart, item, and event. 
  -  Establish effective feature engineering and descriptive modeling that supports interventions to reduce abandonment.

The findings from this analysis will serve as a foundational step in developing interventions that improve MyCoke360’s ability to optimize digital ordering behavior and minimize revenue loss.

```{r message=FALSE, warning=FALSE}
# Import libraries
pacman::p_load(
  tidyverse, skimr, dplyr, jsonlite, stringr, purrr, tidyr, ggplot2, tibble, 
  caret, grf, fixest, lubridate, patchwork, data.table, randomForest, 
  factoextra, mclust, arules, arulesViz, ranger, xgboost, tidytext, 
  rsample, scales, rlang, speedglm, pROC, dbscan, plotly
)

# install.packages("dbscan") 

# Import 
gao <- fread("google_analytics_orders.csv") |>
  dplyr::select(-EVENT_ROW_ID)
materials <- fread("material.csv")
orders <- fread("merged_orders_customer_material.csv")

# Examine the data
gao |>
  head()
```

## Feature Engineering

### Add Order Window ID 

```{r}
# Create function to assign windows
assign_windows <- function(dt) {
  # Ensure data.table
  if (!data.table::is.data.table(dt)) data.table::setDT(dt)

  # 1) Parse timestamps
  dt[, `:=`(
    CUTOFF_TIMESTAMP    = lubridate::ymd_hms(CUTOFF_TIMESTAMP, quiet = TRUE),
    EVENT_TIMESTAMP_ADJ = lubridate::ymd_hms(EVENT_TIMESTAMP_ADJ, quiet = TRUE)
  )]

  # 2) Build windows from DISTINCT cutoffs (per customer)
  ow <- unique(dt[, .(CUSTOMER_ID, CUTOFF_TIMESTAMP)])
  data.table::setorder(ow, CUSTOMER_ID, CUTOFF_TIMESTAMP)
  ow[, `:=`(
    window_start = data.table::shift(CUTOFF_TIMESTAMP),
    window_end   = CUTOFF_TIMESTAMP,
    WINDOW_ID    = seq_len(.N)
  ), by = CUSTOMER_ID]

  # 3) Non-equi join — assign WINDOW_ID and window bounds
  dt[
    ow,
    on = .(CUSTOMER_ID,
           EVENT_TIMESTAMP_ADJ > window_start,
           EVENT_TIMESTAMP_ADJ <= window_end),
    `:=`(WINDOW_ID    = i.WINDOW_ID,
         window_start = i.window_start,
         window_end   = i.window_end)
  ]

  # 4) Reorder columns for clarity
  if ("WINDOW_ID" %in% names(dt)) data.table::setcolorder(dt, c("CUSTOMER_ID", "WINDOW_ID"))

  dt  # return modified table
}

# Run the function on the datset
gao <- assign_windows(gao)


# Create function to assign missing windows
assign_missing_windows <- function(dt) {
  stopifnot(is.data.table(dt))  # ensure it's a data.table
  
  # 1) Compute per-customer bounds from rows that already have a window
  bounds <- dt[!is.na(WINDOW_ID),
    .(
      first_start = min(window_start, na.rm = TRUE),
      last_end    = max(window_end,   na.rm = TRUE),
      last_id     = max(WINDOW_ID,    na.rm = TRUE)
    ),
    by = CUSTOMER_ID
  ]
  
  # 2) If NA and before earliest start -> set WINDOW_ID = 0
  dt[bounds, on = .(CUSTOMER_ID),
     WINDOW_ID := fifelse(is.na(WINDOW_ID) & EVENT_TIMESTAMP_ADJ <= i.first_start, 0L, WINDOW_ID)]
  
  # 3) If NA and after latest end -> set WINDOW_ID = (latest WINDOW_ID + 1)
  dt[bounds, on = .(CUSTOMER_ID),
     WINDOW_ID := fifelse(is.na(WINDOW_ID) & EVENT_TIMESTAMP_ADJ > i.last_end, i.last_id + 1L, WINDOW_ID)]
  
  # 4) Impute any remaining NAs with 0
  dt[is.na(WINDOW_ID), WINDOW_ID := 0L]
  
  # 5) Remove columns
  cols_to_remove <- c("window_start", "window_end")
  dt[, (intersect(cols_to_remove, names(dt))) := NULL]
  
  invisible(dt)  # return silently but modifies in place
}

# Run the function on the datset
gao <- assign_missing_windows(gao)
```

### `CART_ID`

 To further understand cart abandonment, we decided to create a `cart_id` using the unique `customer_id` and `cutoff_timestamp`. This unique cart identifier allowed us to start develop an understanding of cart abandonment at the cart level, rather than just at the customer level. Thus, we were able to better understand the behaviors that led to cart abandonment by creating this analysis variable.
 
```{r}
# Create CART_ID variable
create_cart_id <- function(gao) {
  gao |>

    # 2. Ensure proper timestamp formatting
    mutate(
      EVENT_TIMESTAMP_ADJ = ymd_hms(EVENT_TIMESTAMP_ADJ),
      CUTOFF_TIMESTAMP = ymd_hms(CUTOFF_TIMESTAMP)
    ) |>
    
    # 3. Create a unique cart identifier
    mutate(CART_ID = paste(CUSTOMER_ID, CUTOFF_TIMESTAMP, sep = "_"))
    
}

gao <- create_cart_id(gao)

# Check new CART_ID variable
gao |>
  head()

# Count the total number of CART_IDs
gao |>
  summarize(n_unique_carts = n_distinct(CART_ID)) # 50910

# Sanity check for the number of unique CUSTOMER_ID and CUTOFF_TIMESTAMP combinations
gao |>
  group_by(CUSTOMER_ID, CUTOFF_TIMESTAMP) |>
  summarize(.groups = "drop") |>
  tally() |>
  pull(n) # 50910  
```

### Create Target Variable

During our exploratory data analysis, we created a target variable and calculated an abandonment rate that was focused at the customer level. After shifting to the purchase window as our unit of measure, which is represented by the `CART_ID` above, we decided we needed to recalculate our target variable.

Using the `EVENT_NAME` column, we created a target variable by comparing it to the `EVENT_TIMESTAMP_ADJ` column from the orders table. This timestamp has been adjusted to the local time zone and indicates when an order was created. We compared it to the occurrence of any of the following events in the `EVENT_NAME` column: `add_to_cart`, `update_cart`, or `UpdateCart_Cart_Clicked`. Previously, we used just the `add_to_cart` event, but given that the `google_analytics` data is known to have missed some events, we broadened it to include these other events.

If `EVENT_TIMESTAMP_ADJ` occurred between the last of these events and the `CUTOFF_TIMESTAMP`, the cart was classified as not abandoned. If these events never appeared within that time window, the cart was also considered not abandoned. However, if those events occurred but no `EVENT_TIMESTAMP_ADJ` fell within the purchase window before the cutoff, the cart was classified as abandoned.

```{r}
# define the cart abandonment function
classify_cart_abandonment <- function(gao, orders) {
  # parse order timestamps 
  tz_events <- attr(gao$EVENT_TIMESTAMP_ADJ, "tzone")
  if (is.null(tz_events) || identical(tz_events, "")) tz_events <- "UTC"

  orders_parsed <- orders |>
    mutate(
      CREATED_POSIX = as.POSIXct(
        CREATED_DATE_ADJUSTED,
        format = "%Y-%m-%d %H:%M:%S%z",
        tz = tz_events
      )
    ) |>
    dplyr::select(CUSTOMER_ID, CREATED_POSIX)

  # build one window per cart based on EVENT_NAME activity 
  cart_windows <- gao |>
    filter(EVENT_NAME %in% c("add_to_cart", "update_cart", "UpdateCart_Cart_Clicked")) |> # events that we consider creating a cart
    group_by(CART_ID, CUSTOMER_ID) |>
    summarise(
      window_start = max(EVENT_TIMESTAMP_ADJ, na.rm = TRUE),  # last relevant event
      window_end   = suppressWarnings(max(CUTOFF_TIMESTAMP, na.rm = TRUE)),
      .groups = "drop"
    ) |>
    filter(!is.na(window_end), window_start <= window_end)

  # match each cart window to orders by CUSTOMER_ID to classify abandonment 
  cart_classified <- cart_windows |>
    left_join(orders_parsed, by = "CUSTOMER_ID") |>
    mutate(
      in_window = !is.na(CREATED_POSIX) &
                  CREATED_POSIX >= window_start &
                  CREATED_POSIX <= window_end
    ) |>
    group_by(CART_ID) |>
    summarise(ABANDONED = as.integer(!any(in_window, na.rm = TRUE)), .groups = "drop")

  # add the ABANDONED target variable back to every event row 
  gao_with_target <- gao |>
    left_join(cart_classified, by = "CART_ID") |>
    mutate(ABANDONED = ifelse(is.na(ABANDONED), 0L, ABANDONED))  # treat NAs as not abandoned

  # print summary 
  summary_counts <- gao_with_target |>
    summarise(
      total_carts = n_distinct(CART_ID),
      abandoned_carts = n_distinct(CART_ID[ABANDONED == 1]),
      not_abandoned_carts = n_distinct(CART_ID[ABANDONED == 0]),
      pct_abandoned = round(100 * abandoned_carts / total_carts, 2),
      pct_not_abandoned = round(100 * not_abandoned_carts / total_carts, 2)
    )

  cat("\n CART ABANDONMENT SUMMARY\n")
  print(summary_counts)
  cat("\n✅ The returned data frame contains", nrow(gao_with_target),
      "rows — identical to the input gao.\n")

  # return the full event-level dataset with target variable 
  gao_with_target
}


# call function and create new data set
gao <- classify_cart_abandonment(gao, orders)
```

Our number of carts stayed the same, as did our total number of observations. Of our 50,910 carts, a total of 7,539 were abandoned, giving us an abandonment rate of 14.81%.

### Create TOTAL_ITEMS

We wanted to evaluate how the number of items affected cart abandonment so we created a function to parse and calculate the number of items from the json string in the ITEMS column. This items column resulted in us being able to count the total number of items in a cart at any given time and was critical to our analysis. 

```{r}
sum_cart_quantities <- function(df, json_col, out_col = "TOTAL_ITEMS") {
  
  # Convert strings to symbols for tidy evaluation
  json_col <- rlang::ensym(json_col)
  out_col <- rlang::ensym(out_col)
  
  # Start timer
  start_time <- Sys.time()
  
  message("⏳ Parsing JSON and summing quantities...")
  
  # Apply vectorized parsing logic
  result <- df %>%
    mutate(
      !!out_col := sapply(!!json_col, function(x) {
        # Handle empty / NA cases quickly
        if (is.na(x) || x == "" || x == "[]") return(0)
        
        # Fix double-double quotes
        x_clean <- gsub('""', '"', x, fixed = TRUE)
        
        # Safely parse JSON
        items <- tryCatch(fromJSON(x_clean), error = function(e) NULL)
        
        # Return 0 if JSON invalid or doesn't have quantity
        if (is.null(items) || !"quantity" %in% names(items)) return(0)
        
        # Sum quantities
        sum(as.numeric(items$quantity), na.rm = TRUE)
      })
    )
  
  # End timer
  end_time <- Sys.time()
  message(sprintf("✅ Done in %.2f seconds", as.numeric(difftime(end_time, start_time, units = "secs"))))
  
  return(result)
}

# Apply to gao
gao <- sum_cart_quantities(gao, ITEMS, TOTAL_ITEMS)

# Show TOTAL_ITEMS distribution
gao |>
  ggplot(aes(x = TOTAL_ITEMS)) +
  geom_histogram(bins = 100, fill = "#BB021E", color = "black", alpha = 0.8) +
  labs(
    title = "Distribution of Total Items per Cart",
    x = "Total Items",
    y = "Count"
  ) +
  coord_cartesian(xlim = c(0, 1000)) +
  theme_minimal()

# Bin total items properly into ITEM_BIN with fixed levels
bin_levels <- c("1–9", "10–99", "100–999", "1K–9K", "10K–99K", "100K+")
gao <- gao |>
  mutate(
    ITEM_BIN = cut(
      TOTAL_ITEMS,
      breaks = c(0, 10, 100, 1000, 10000, 100000, Inf),
      labels = bin_levels,
      include.lowest = TRUE,
      right = FALSE
    ),
    ITEM_BIN = factor(ITEM_BIN, levels = bin_levels)  # <- this keeps *all* six bins
  )

# Plot ITEM_BIN variable
gao |>
  ggplot(aes(x = ITEM_BIN)) +
  geom_bar(fill = "#BB021E", color = "black", alpha = 0.8) +
  labs(
    title = "Distribution of Total Items per Cart",
    x = "Total Items",
    y = "Count"
  ) +
  theme_minimal()
```

### Add Days Since Last Purchased Field

To better understand if the recency of a purchase affects cart abandonment, we engineered a `days_since_last_purchase` variable. This feature measures the number of days between an event and the most recent purchase, providing a proxy variable for consumer engagement. Unfortunately, due to the short time window of this dataset, many customers do not have a prior recorded purchase. In this case, the variable is recorded as `NA` to signify the lack of history. This variable will help us distinguish between new and repeat customers without introducing data leakage via a binary feature.

```{r}
add_days_since_last_purchase <- function(df, customer_col = CUSTOMER_ID, 
                                         event_col = EVENT_TIMESTAMP_ADJ, 
                                         purchase_flag = PURCHASE_EVENT, 
                                         out_col = DAYS_SINCE_LAST_PURCHASE) {
  # Convert to quosures for tidy evaluation
  customer_col <- enquo(customer_col)
  event_col <- enquo(event_col)
  purchase_flag <- enquo(purchase_flag)
  out_col <- enquo(out_col)
  
  df |>
    arrange(!!customer_col, !!event_col) |>
    group_by(!!customer_col) |>
    mutate(
      # Capture last purchase date for each event
      last_purchase_date = if_else(!!purchase_flag == 1, !!event_col, NA),
      last_purchase_date = as_datetime(zoo::na.locf(last_purchase_date, na.rm = FALSE)),
      # Compute days difference
      !!out_col := floor(as.numeric(difftime(!!event_col, last_purchase_date, units = "days"))
    )) |>
    ungroup() |>
    dplyr::select(-last_purchase_date)
}

gao <- gao |>
  mutate(PURCHASE_EVENT = if_else(ABANDONED == 0, 1, 0)) |>
  add_days_since_last_purchase()
```

```{r}
# remove old target variable column from EDA to avoid confusion
gao <- gao |>
  dplyr::select(-ABANDONED_CART)

# check to make sure removal worked.
head(gao)
```

## Analysis

### Items in Abandoned Carts Analysis

In an attempt to answer the business question, “Which products appear most frequently in abandoned carts?”, we wanted to find out if there were any specific items or category of items, that are correlated with cart abandonment. So we decided to calculate the number of carts that each item appears in, the number of abandoned carts that have each item, and the rate of abandonment for each item.

Looking at different carts, the items listed in a cart often appear multiple times. To mitigate this, we used the last time the ITEMS column appears with products in it for each individual cart. We also joined the cart back with the materials table so that we could look at categories such as pack size/type, brand, and flavor. 

```{r}
compute_item_abandonment <- function(gao, materials) {

  item_abandonment <- gao |>
    # Keep only last entry per cart
    group_by(CART_ID) |>
    filter(EVENT_TIMESTAMP_ADJ == max(EVENT_TIMESTAMP_ADJ, na.rm = TRUE)) |>
    ungroup() |>
    
    # Parse JSON safely
    mutate(ITEMS_PARSED = purrr::map(ITEMS, function(x) {
      if (is.na(x) || x == "" || x == "[]") return(NULL)
      
      # Fix invalid double quotes
      x_clean <- gsub('""', '"', x, fixed = TRUE)
      
      # Try parsing JSON; return NULL if invalid
      parsed <- tryCatch(fromJSON(x_clean), error = function(e) NULL)
      return(parsed)
    })) |>
    
    # Unnest only non-empty lists
    filter(!map_lgl(ITEMS_PARSED, is.null)) |>
    unnest(ITEMS_PARSED) |>
    
    dplyr::select(CART_ID, ITEM_ID = item_id, ABANDONED) |>
    distinct(CART_ID, ITEM_ID, .keep_all = TRUE) |>
    
    # Aggregate by item
    group_by(ITEM_ID) |>
    summarise(
      CARTS_WITH_ITEM = n_distinct(CART_ID),
      ABANDONED_CARTS_WITH_ITEM = n_distinct(CART_ID[ABANDONED == 1]),
      NON_ABANDONED_CARTS_WITH_ITEM = n_distinct(CART_ID[ABANDONED == 0]),
      ABANDONMENT_RATE = ABANDONED_CARTS_WITH_ITEM / CARTS_WITH_ITEM,
      NON_ABANDONED_RATE = NON_ABANDONED_CARTS_WITH_ITEM / CARTS_WITH_ITEM,
      .groups = "drop"
    ) |>
    
    # Join to materials
    mutate(ITEM_ID = suppressWarnings(as.integer(ITEM_ID))) |>
    inner_join(materials, by = c("ITEM_ID" = "MATERIAL_ID"))
  
  return(item_abandonment)
}

item_abandonment <- compute_item_abandonment(gao, materials)

# display table
item_abandonment |>
  arrange(desc(ABANDONMENT_RATE)) |>
  slice_head(n = 50)
```

This table shows that there are a few items with very high abandonment rates, but the overall volume per item can be very low. In order to get a better idea of which items are in a higher number of carts and may also have a higher abandonment rate, we decided to create some visuals and tables to help weigh the rate vs volume issue.

#### Top Items Appearing in Abandoned Carts

 Our baseline abandonment rate is approximately 15%, so we decided to take a look at the items that had an abandonment rate above that baseline.

```{r}
# filter for items with an abandonment rate above 15%
top_n_items <- item_abandonment |>
  filter(ABANDONMENT_RATE >= 0.15) |>
  arrange(desc(ABANDONED_CARTS_WITH_ITEM)) |>
  slice_head(n = 20)

# max rate in this plot
max_rate <- max(top_n_items$ABANDONMENT_RATE, na.rm = TRUE)

# plot with legend title showing the max
ggplot(top_n_items, aes(x = reorder(ITEM_ID, ABANDONED_CARTS_WITH_ITEM),
                        y = ABANDONED_CARTS_WITH_ITEM,
                        fill = ABANDONMENT_RATE)) +
  geom_col() +
  coord_flip() +
  scale_fill_gradient(
    low = "white",
    high = "#BB021E",
    labels = label_percent(accuracy = 1)  
  ) +
  labs(
    title = "Top 20 Items (Abandonment Rate ≥ 15%)",
    x = "Item SKU",
    y = "Number of Abandoned Carts",
    fill = paste0("Abandonment Rate\n(max: ", percent(max_rate, accuracy = 0.1), ")")
  ) +
  theme_minimal(base_size = 13) +
  theme(
    plot.background = element_rect(fill = "#1C1C1C", color = NA),
    panel.background = element_rect(fill = "#1C1C1C", color = NA),
    panel.grid.major.x = element_line(color = "gray30"),
    panel.grid.major.y = element_blank(),
    panel.grid.minor = element_blank(),
    axis.text = element_text(color = "white"),
    axis.title = element_text(color = "white"),
    plot.title = element_text(size = 14, face = "bold", color = "white"),
    legend.text = element_text(color = "white"),
    legend.title = element_text(color = "white")
  )

# display table
top_n_items
```

All of these items have an abandonment rate of 15% or higher. SKU 413074, Grandma's Pantry Ginger Chai, stands out for having an abandonment rate of 21%, and it appears in approximately 78 abandoned carts. 

#### Top Flavors Appearing in Abandoned Carts

Next we decided to look at abandonment rates based on the different categories starting with flavor.

```{r}
# aggregate by flavor, keep only ≥15 % abandonment
flavor_abandonment <- item_abandonment |>
  group_by(FLAVOUR_DESC) |>
  summarise(
    TOTAL_CARTS_WITH_FLAVOUR = sum(CARTS_WITH_ITEM, na.rm = TRUE),
    TOTAL_ABANDONED_CARTS_WITH_FLAVOUR = sum(ABANDONED_CARTS_WITH_ITEM, na.rm = TRUE),
    ABANDONMENT_RATE = TOTAL_ABANDONED_CARTS_WITH_FLAVOUR / TOTAL_CARTS_WITH_FLAVOUR,
    .groups = "drop"
  ) |>
  filter(ABANDONMENT_RATE >= 0.15) |>                     
  arrange(desc(TOTAL_ABANDONED_CARTS_WITH_FLAVOUR))

# Plot top 15 flavors
ggplot(flavor_abandonment |> slice_head(n = 15),
       aes(x = reorder(FLAVOUR_DESC, TOTAL_ABANDONED_CARTS_WITH_FLAVOUR),
           y = TOTAL_ABANDONED_CARTS_WITH_FLAVOUR,
           fill = ABANDONMENT_RATE)) +
  geom_col() +
  coord_flip() +
  scale_fill_gradient(low = "white", high = "#BB021E") +
  labs(
    title = "Top Flavors (Abandonment Rate ≥ 15%)",
    x = "Flavour",
    y = "Number of Abandoned Carts",
    fill = "Abandonment Rate"
  ) +
  theme_minimal(base_size = 13) +
  theme(
    plot.background = element_rect(fill = "#1C1C1C", color = NA),
    panel.background = element_rect(fill = "#1C1C1C", color = NA),
    panel.grid.major.x = element_line(color = "gray30"),  # keep vertical grid lines
    panel.grid.major.y = element_blank(),                 # remove horizontal grid lines
    panel.grid.minor = element_blank(),
    axis.text = element_text(color = "white"),
    axis.title = element_text(color = "white"),
    plot.title = element_text(size = 14, face = "bold", color = "white"),
    plot.subtitle = element_text(color = "white"),
    legend.text = element_text(color = "white"),
    legend.title = element_text(color = "white")
  )

# display table
head(flavor_abandonment, 15)
```

What stands out here is that there are a few flavors, such as Vanilla Raspberry, that are present in a large number of abandoned carts and, when they are put into a cart, are abandoned at a higher rate than our baseline.

#### Top Pack Sizes Appearing in Abandoned Carts

```{r}
# aggregate by pack size and filter for ≥15% abandonment
packsize_abandonment <- item_abandonment |>
  mutate(PACK_SIZE_DESC = ifelse(is.na(PACK_SIZE_DESC) | PACK_SIZE_DESC == "", "Unknown", PACK_SIZE_DESC)) |>
  group_by(PACK_SIZE_DESC) |>
  summarise(
    TOTAL_CARTS_WITH_PACKSIZE = sum(CARTS_WITH_ITEM, na.rm = TRUE),
    TOTAL_ABANDONED_CARTS_WITH_PACKSIZE = sum(ABANDONED_CARTS_WITH_ITEM, na.rm = TRUE),
    ABANDONMENT_RATE = TOTAL_ABANDONED_CARTS_WITH_PACKSIZE / TOTAL_CARTS_WITH_PACKSIZE,
    .groups = "drop"
  ) |>
  filter(ABANDONMENT_RATE >= 0.15) |>                     
  arrange(desc(TOTAL_ABANDONED_CARTS_WITH_PACKSIZE))

# Plot top 15 pack sizes
ggplot(packsize_abandonment |> slice_head(n = 15),
       aes(x = reorder(PACK_SIZE_DESC, TOTAL_ABANDONED_CARTS_WITH_PACKSIZE),
           y = TOTAL_ABANDONED_CARTS_WITH_PACKSIZE,
           fill = ABANDONMENT_RATE)) +
  geom_col() +
  coord_flip() +
  scale_fill_gradient(low = "white", high = "#BB021E") +
  labs(
    title = "Top Pack Sizes (Abandonment Rate ≥ 15%)",
    x = "Pack Size",
    y = "Number of Abandoned Carts",
    fill = "Abandonment Rate"
  ) +
  theme_minimal(base_size = 13) +
  theme(
    plot.background = element_rect(fill = "#1C1C1C", color = NA),
    panel.background = element_rect(fill = "#1C1C1C", color = NA),
    panel.grid.major.x = element_line(color = "gray30"),  
    panel.grid.major.y = element_blank(),                 
    panel.grid.minor = element_blank(),
    axis.text = element_text(color = "white"),
    axis.title = element_text(color = "white"),
    plot.title = element_text(size = 14, face = "bold", color = "white"),
    plot.subtitle = element_text(color = "white"),
    legend.text = element_text(color = "white"),
    legend.title = element_text(color = "white")
  )

# display table
head(packsize_abandonment, 15)
```

While the 20 OZ size does appear in a large number of abandoned carts, that is probably due to it being in a large proportion of all carts. 

#### Top Beverage Categories Appearing in Abandoned Carts

```{r}
# summarize item-level abandonment into beverage-category level
bevcat_abandonment <- item_abandonment |>
  group_by(BEV_CAT_DESC) |>
  summarise(
    TOTAL_CARTS_WITH_BEV_CAT = sum(CARTS_WITH_ITEM, na.rm = TRUE),
    TOTAL_ABANDONED_CARTS_WITH_BEV_CAT = sum(ABANDONED_CARTS_WITH_ITEM, na.rm = TRUE),
    ABANDONMENT_RATE = TOTAL_ABANDONED_CARTS_WITH_BEV_CAT / TOTAL_CARTS_WITH_BEV_CAT
  ) |>
  arrange(desc(ABANDONMENT_RATE)) |>
  filter(ABANDONMENT_RATE >= 0.15)

# plot top 15 beverage categories
ggplot(bevcat_abandonment |> slice_head(n = 15),
       aes(x = reorder(BEV_CAT_DESC, TOTAL_ABANDONED_CARTS_WITH_BEV_CAT),
           y = TOTAL_ABANDONED_CARTS_WITH_BEV_CAT,
           fill = ABANDONMENT_RATE)) +
  geom_col() +
  coord_flip() +
  scale_fill_gradient(low = "white", high = "#BB021E") +
  labs(
    title = "Abandoned  Beverage Categories",
    subtitle = "Abandonment Rate ≥ 15%",
    x = "Beverage Category",
    y = "Number of Abandoned Carts",
    fill = "Abandonment Rate"
  ) +
  theme_minimal(base_size = 13) +
  theme(
    plot.background  = element_rect(fill = "#1C1C1C", color = NA),
    panel.background = element_rect(fill = "#1C1C1C", color = NA),
    panel.grid.major.x = element_line(color = "gray30"),  
    panel.grid.major.y = element_blank(),
    panel.grid.minor   = element_blank(),
    axis.text.y = element_text(
      color = "white",
      angle = 30,       # diagonal angle for long category names
      hjust = 1,
      size = 6          
    ),
    axis.text.x  = element_text(color = "white", size = 9),
    axis.title   = element_text(color = "white"),
    plot.title   = element_text(size = 14, face = "bold", color = "white", hjust = 0),
    plot.subtitle= element_text(color = "white"),
    legend.text  = element_text(color = "white"),
    legend.title = element_text(color = "white"),
    plot.margin  = ggplot2::margin(t = 10, r = 40, b = 10, l = 10)
  )

 # display table
head(bevcat_abandonment, 15)
```

Core Sparkling appears in the highest number of abandoned carts, with an abandonment rate of 15.8%. Enhance Water Beverages has an abandonment rate of 23.11% and appears in 230 abandoned carts.

#### Top Brands Appearing in Abandoned Carts

```{r}
# aggregate by trade mark / brand and filter for ≥15% abandonment
trademark_abandonment <- item_abandonment |>
  mutate(TRADE_MARK_DESC = ifelse(is.na(TRADE_MARK_DESC) | TRADE_MARK_DESC == "", "Unknown", TRADE_MARK_DESC)) |>
  group_by(TRADE_MARK_DESC) |>
  summarise(
    TOTAL_CARTS_WITH_TRADEMARK = sum(CARTS_WITH_ITEM, na.rm = TRUE),
    TOTAL_ABANDONED_CARTS_WITH_TRADEMARK = sum(ABANDONED_CARTS_WITH_ITEM, na.rm = TRUE),
    ABANDONMENT_RATE = TOTAL_ABANDONED_CARTS_WITH_TRADEMARK / TOTAL_CARTS_WITH_TRADEMARK,
    .groups = "drop"
  ) |>
  filter(ABANDONMENT_RATE >= 0.15) |>                     
  arrange(desc(TOTAL_ABANDONED_CARTS_WITH_TRADEMARK))

# plot top 15 trade marks
ggplot(trademark_abandonment |> slice_head(n = 15),
       aes(x = reorder(TRADE_MARK_DESC, TOTAL_ABANDONED_CARTS_WITH_TRADEMARK),
           y = TOTAL_ABANDONED_CARTS_WITH_TRADEMARK,
           fill = ABANDONMENT_RATE)) +
  geom_col() +
  coord_flip() +
  scale_fill_gradient(low = "white", high = "#BB021E") +
  labs(
    title = "Top Trade Marks (Abandonment Rate ≥ 15%)",
    x = "Trade Mark / Brand",
    y = "Number of Abandoned Carts",
    fill = "Abandonment Rate"
  ) +
  theme_minimal(base_size = 13) +
  theme(
    plot.background = element_rect(fill = "#1C1C1C", color = NA),
    panel.background = element_rect(fill = "#1C1C1C", color = NA),
    panel.grid.major.x = element_line(color = "gray30"),   
    panel.grid.major.y = element_blank(),                  
    panel.grid.minor = element_blank(),
    axis.text = element_text(color = "white", size = 9),
    axis.title = element_text(color = "white"),
    plot.title = element_text(size = 14, face = "bold", color = "white", hjust = 0),  
    plot.subtitle = element_text(color = "white"),
    legend.text = element_text(color = "white"),
    legend.title = element_text(color = "white"),
    plot.margin = ggplot2::margin(t = 10, r = 40, b = 10, l = 10)
  )

# display table
head(trademark_abandonment, 15)
```

At a 16.64% abandonment rate, Fizz Factory’s abandonment rate is only slightly above our 15% baseline, but it is present in a large number of abandoned carts at 3,334.

#### Top Pack Types Appearing in Abandoned Carts

```{r}
# Aggregate by pack type and filter for ≥15% abandonment
packtype_abandonment <- item_abandonment |>
  mutate(PACK_TYPE_DESC = ifelse(is.na(PACK_TYPE_DESC) | PACK_TYPE_DESC == "", "Unknown", PACK_TYPE_DESC)) |>
  group_by(PACK_TYPE_DESC) |>
  summarise(
    TOTAL_CARTS_WITH_PACKTYPE = sum(CARTS_WITH_ITEM, na.rm = TRUE),
    TOTAL_ABANDONED_CARTS_WITH_PACKTYPE = sum(ABANDONED_CARTS_WITH_ITEM, na.rm = TRUE),
    ABANDONMENT_RATE = TOTAL_ABANDONED_CARTS_WITH_PACKTYPE / TOTAL_CARTS_WITH_PACKTYPE,
    .groups = "drop"
  ) |>
  filter(ABANDONMENT_RATE >= 0.15) |>                     
  arrange(desc(TOTAL_ABANDONED_CARTS_WITH_PACKTYPE))

# Plot top 15 pack types
ggplot(packtype_abandonment |> slice_head(n = 15),
       aes(x = reorder(PACK_TYPE_DESC, TOTAL_ABANDONED_CARTS_WITH_PACKTYPE),
           y = TOTAL_ABANDONED_CARTS_WITH_PACKTYPE,
           fill = ABANDONMENT_RATE)) +
  geom_col() +
  coord_flip() +
  scale_fill_gradient(low = "white", high = "#BB021E") +
  labs(
    title = "Top Pack Types (Abandonment Rate ≥ 15%)",
    x = "Pack Type",
    y = "Number of Abandoned Carts",
    fill = "Abandonment Rate"
  ) +
  theme_minimal(base_size = 13) +
  theme(
    plot.background = element_rect(fill = "#1C1C1C", color = NA),
    panel.background = element_rect(fill = "#1C1C1C", color = NA),
    panel.grid.major.x = element_line(color = "gray30"),   
    panel.grid.major.y = element_blank(),                  
    panel.grid.minor = element_blank(),
    axis.text = element_text(color = "white", size = 9),
    axis.title = element_text(color = "white"),
    plot.title = element_text(size = 14, face = "bold", color = "white", hjust = 0),  
    plot.subtitle = element_text(color = "white"),
    legend.text = element_text(color = "white"),
    legend.title = element_text(color = "white"),
    plot.margin = ggplot2::margin(t = 10, r = 40, b = 10, l = 10)
  )

# display table
packtype_abandonment
```

It is difficult to tell if any of the pack types are potentially contributing to cart abandonment just by looking at the numbers. Taking into account that these are beverages or beverage-related products, it would make sense that the different materials, such as plastic, glass, or aluminum, would appear in a lot of abandoned carts. More modeling would be needed to gain deeper insight.


## Events and Abandoned Carts

Another business question was, “What behavioral events or sequence of events are the strongest predictors of cart abandonment?” We thought that `EVENT_NAME` had the best potential to help us answer this question. This column captures up to 148 different events that may occur during a particular purchase window. Many of these events can occur multiple times, such as `button_click`. Others may show up rarely or not at all.

The other issue we had to consider was how the behavior of a large-volume customer, such as a wholesaler or chain retailer, might overpower the behavior of a smaller-volume customer, such as a standalone small restaurant. Taking this into consideration, we decided to compute the proportion of each individual event type that occurred in each purchase window. So, if there were 10 events and 3 of them were `button_click`, then `button_click` would have a score of 0.30. If a specific type of event did not occur, then it would have a score of 0. This way, all of our observations would keep the same dimensions without blank or missing data.

The end result was a single row per `CART_ID`, 50,910 total rows, our target variable `ABANDON`, and 148 columns for all of the unique event names.

This table does not contain any other data, but using `CART_ID`, it could be merged back with the aggregated table containing the other data we have. 

```{r}
# define function for normalizing events by proportion
normalize_events_by_cart <- function(gao) {
  # 1) Exclude the target-related events
  excluded_events <- c("add_to_cart", "update_cart", "UpdateCart_Cart_Clicked")
  filtered_events <- gao |>
    filter(!EVENT_NAME %in% excluded_events)

  # count how many times each event occurs in a cart
  event_counts <- filtered_events |>
    group_by(CART_ID, EVENT_NAME) |>
    summarise(event_count = n(), .groups = "drop")

  # normalize counts to proportions within each cart
  normalized_events <- event_counts |>
    group_by(CART_ID) |>
    mutate(total_events = sum(event_count),
           prop = event_count / total_events) |>
    ungroup()

  # pivot to wide format — one column per event
  event_wide <- normalized_events |>
    dplyr::select(CART_ID, EVENT_NAME, prop) |>
    pivot_wider(names_from = EVENT_NAME,
                values_from = prop,
                values_fill = 0)

  # add target variable (ABANDONED)
  cart_target <- gao |>
    distinct(CART_ID, ABANDONED)

  final_data <- cart_target |>
    left_join(event_wide, by = "CART_ID") |>
    mutate(across(where(is.numeric), ~ replace_na(.x, 0)))

  # return final dataset
  final_data
}

# apply to gao
event_normalized <- normalize_events_by_cart(gao)

# display table
head(event_normalized)
```

```{r}
# convert to long for comparison
events_long <- event_normalized |>
  pivot_longer(
    cols = -c(CART_ID, ABANDONED),
    names_to = "EVENT_NAME",
    values_to = "prop"
  )

# mean proportion of each event by abandonment group
event_group_means <- events_long |>
  group_by(ABANDONED, EVENT_NAME) |>
  summarise(
    mean_prop = mean(prop, na.rm = TRUE),
    carts_with_event = sum(prop > 0, na.rm = TRUE),
    .groups = "drop"
  )

# “Top-N” events per group by mean proportion
topN <- 10  # adjust as needed
top_abandoned <- event_group_means |>
  filter(ABANDONED == 1) |>
  arrange(desc(mean_prop)) |>
  slice_head(n = topN)

top_not_abandoned <- event_group_means |>
  filter(ABANDONED == 0) |>
  arrange(desc(mean_prop)) |>
  slice_head(n = topN)

# events with the biggest difference (abandoned minus not-abandoned)
event_lift <- event_group_means |>
  pivot_wider(names_from = ABANDONED, values_from = c(mean_prop, carts_with_event),
              names_prefix = "abnd_") |>
  # columns are now mean_prop_abnd_0, mean_prop_abnd_1, etc.
  mutate(
    diff_mean_prop = mean_prop_abnd_1 - mean_prop_abnd_0,
    abs_diff = abs(diff_mean_prop)
  ) |>
  arrange(desc(abs_diff))

# events with largest differences between groups (Top-N by absolute diff)
ggplot(event_lift |> slice_max(abs_diff, n = topN),
       aes(x = reorder(EVENT_NAME, abs_diff),
           y = diff_mean_prop,
           fill = diff_mean_prop > 0)) +
  geom_col() +
  coord_flip() +
  scale_fill_manual(
    values = c("TRUE" = "#BB021E", "FALSE" = "white"),
    labels = c("TRUE" = "Abandoned Carts",
               "FALSE" = "Non-Abandoned Carts"),
    name = NULL   
  ) +
  labs(
    title = "Difference in Mean Proportion",
    x = "Event",
    y = "Difference in Mean Proportion (Abandoned - Not Abandoned)"
  ) +
  theme_minimal(base_size = 13) +
  theme(
    plot.background = element_rect(fill = "#1C1C1C", color = NA),
    panel.background = element_rect(fill = "#1C1C1C", color = NA),
    panel.grid.major.x = element_line(color = "gray30"),
    panel.grid.major.y = element_blank(),
    panel.grid.minor = element_blank(),
    axis.text = element_text(color = "white", size = 9),
    axis.title = element_text(color = "white"),
    plot.title = element_text(size = 14, face = "bold", color = "white", hjust = 0),
    legend.text = element_text(color = "white"),
    plot.margin = ggplot2::margin(t = 10, r = 40, b = 10, l = 10),
    legend.position = "bottom"
  )

top_abandoned
top_not_abandoned
```

The two tables list the events that have the largest mean proportion of abandoned and not-abandoned carts. There is a lot of overlap in the types of behaviors that lead up to a cart being abandoned or not. The chart does seem to indicate that events such as `view_item_list` occur a larger proportion of the time in abandoned carts. Overall, these differences appear to be fairly small.

## Modeling

### Attempted Modeling Efforts

**Random Forest**

Prior to our mentorship meetings with Professor Webb, we attempted a random forest model. However, as discussed in our **Summary of Findings** section, we experienced severe target variable leakage due to not removing the variables and rows that we used to create the original cart abandonment variable. Additionally, the random forest model was taking a very long time to run and after speaking with Professor Webb, we decided that such a complicated model was unnecessary due to our focus on behavioral descriptions over prediction.

**Interrupted Time Series**

We realized after doing some more investigation that this modeling approach wouldn't work because there is no way to find cart abandonment data before the treatment period because cart abandonment can only be calculated by google analytics data which is only available for the treatment period. We originally wanted to do causal modeling like 1WFE and time series but neither of these approaches worked because of the lack of cart abandonment data prior to the intervention.

**Gaussian Mixture Models (GMM)**

We attempted to identify latent behavioral segments within 500 sampled carts using Gaussian Mixture Model, but results showed significant overlap among the seven inferred clusters. Our model attempted to fit a variable volume, variable shape, axis aligned structure with minimal separation between groups, indicating that behavioral variation in event proportions is largely continuous rather than multimodal. This reinforces earlier PCA and hierarchical clustering findings that MyCoke360 users do not form distinct behavioral types but instead differ by degree of engagement or interaction intensity.

### Hierarchical Clustering

```{r hierarchical_clustering_sample500, message=FALSE, warning=FALSE}
# Prepare a 500-cart random sample from the normalized event data
event_features <- event_normalized %>%
  dplyr::select(-CART_ID, -ABANDONED)

set.seed(123)
sample_index <- sample(nrow(event_features), 500)
event_sample <- event_features[sample_index, ]

# Remove features with zero variance to ensure valid distance calculations
event_sample <- event_sample[, sapply(event_sample, function(x) sd(x, na.rm = TRUE) != 0)]

# Standardize all variables before computing distances
event_scaled <- scale(event_sample)

# Compute the distance matrix and perform hierarchical clustering
dist_matrix <- dist(event_scaled, method = "euclidean")
hc <- hclust(dist_matrix, method = "ward.D2")

# Evaluate the optimal number of clusters using the silhouette method
fviz_nbclust(event_scaled, hcut, method = "silhouette") +
  labs(title = "Optimal Number of Clusters (Sample of 500)")

# Visualize the hierarchical structure as a dendrogram
fviz_dend(
  hc,
  k = 4,
  cex = 0.6,
  color_labels_by_k = TRUE,
  rect = TRUE,
  main = "Hierarchical Clustering on 500-Cart Sample"
)

# Assign cluster labels to each observation
clusters <- cutree(hc, k = 4)
event_clusters <- event_sample %>%
  mutate(CLUSTER = factor(clusters))

# Visualize the resulting clusters in feature space
fviz_cluster(
  list(data = event_scaled, cluster = clusters),
  geom = "point",
  ellipse.type = "norm",
  ggtheme = theme_minimal(),
  main = "Behavioral Clusters (500-Cart Sample)"
)
```

The hierarchical clustering analysis on a 500-cart sample revealed that MyCoke360 users exhibit subtle but distinguishable behavioral groupings rather than sharply defined segments. Four clusters emerged using Ward’s method: a dense central group representing typical, consistent user behavior; a larger, more diffuse group capturing variable browsing patterns; and two smaller, peripheral clusters likely representing edge-case behaviors such as rapid checkouts or unusually high event activity. Although some separation is visible—suggesting modest differences in how users engage with cart and product-related events—the overlap between clusters indicates that digital ordering behavior is largely continuous. These results reinforce the insight from PCA that cart activity patterns vary by degree rather than by discrete customer types, supporting the use of predictive modeling and behavioral feature analysis over hard segmentation for understanding and reducing cart abandonment.

### DBSCAN Clustering

Next we decided to try the DBSCAN clustering method to see if it could provide insight into what events may contribute to cart abandonment rate. 

```{r}
# define function to run dbscan, with adjustable inputs
run_pca_dbscan <- function(
  data,
  n_top_cols = 20,          # keep top-N columns by mean
  n_pcs = 10,               # PCs to retain for DBSCAN
  n_contrib_vars = 10,      # top PCA contributors shown in var plot
  k_nn = 5,                 # k for eps elbow
  minPts = 10,              # DBSCAN minPts
  small_cluster_prop = 0.01,
  seed = 123
) {
  stopifnot(is.data.frame(data))
  set.seed(seed)

  # numeric only
  data_num <- data[, sapply(data, is.numeric), drop = FALSE]

  # top-N by mean
  means <- colMeans(data_num, na.rm = TRUE)
  keep_cols <- names(sort(means, decreasing = TRUE))[seq_len(min(n_top_cols, length(means)))]
  data_top  <- data_num[, keep_cols, drop = FALSE]

  # scale
  data_scaled <- scale(data_top)

  # PCA
  pca <- prcomp(data_scaled, center = TRUE, scale. = TRUE)
  n_pcs <- min(n_pcs, ncol(pca$x))
  reduced_data <- pca$x[, seq_len(n_pcs), drop = FALSE]
  pca_vis      <- pca$x[, 1:2, drop = FALSE]

  # PCA variable plot 
  pca_var_plot <- fviz_pca_var(
    pca,
    col.var = "white",
    repel = TRUE,
    pointsize = 3,
    title = sprintf("PCA Variable Plot (Top %d Contributing Variables)", n_contrib_vars),
    select.var = list(contrib = n_contrib_vars)
  ) +
    theme_minimal(base_size = 13) +
    theme(
      plot.background = element_rect(fill = "#1C1C1C", color = NA),
      panel.background = element_rect(fill = "#1C1C1C", color = NA),
      panel.grid.major.x = element_line(color = "gray30"),
      panel.grid.major.y = element_blank(),
      panel.grid.minor = element_blank(),
      axis.text  = element_text(color = "white", size = 9),
      axis.title = element_text(color = "white"),
      plot.title = element_text(size = 14, face = "bold", color = "white", hjust = 0),
      legend.text  = element_text(color = "white"),
      legend.title = element_text(color = "white"),
      legend.position = "bottom",
      plot.margin = ggplot2::margin(t = 10, r = 40, b = 10, l = 10)
    )

  # Display PCA variable contribution plot
  print(pca_var_plot)

  # eps elbow
  find_eps_elbow <- function(Z, k = 5) {
    d <- sort(kNNdist(Z, k = k))
    n <- length(d)
    x_sc <- (seq_len(n) - 1)/(n - 1)
    y_sc <- (d - min(d))/(max(d) - min(d))
    eps <- d[which.max(abs(y_sc - x_sc))]
    eps
  }
  best_eps <- find_eps_elbow(reduced_data, k = k_nn)

  # DBSCAN
  db <- dbscan::dbscan(reduced_data, eps = best_eps, minPts = minPts)
  labels <- db$cluster

  # filter micro-clusters into noise 
  cl_sizes <- table(labels)
  size_thresh <- ceiling(small_cluster_prop * nrow(reduced_data))
  large_ids <- as.integer(names(cl_sizes[cl_sizes >= size_thresh]))
  filtered_labels <- ifelse(labels %in% large_ids, labels, 0L)

  # diagnostics: clusters and silhoutte
  n_clusters <- length(unique(filtered_labels[filtered_labels > 0]))
  n_noise    <- sum(filtered_labels == 0)

  if (n_clusters > 1) {
    n_for_sil <- min(5000, nrow(reduced_data))
    sample_idx <- sample(seq_len(nrow(reduced_data)), n_for_sil)
    sil <- cluster::silhouette(filtered_labels[sample_idx],
                               dist(reduced_data[sample_idx, ]))
    silhouette_score <- mean(sil[, 3])
  } else {
    silhouette_score <- NA_real_
  }

  cat("\nDBSCAN Summary\n",
      "  eps: ", round(best_eps, 4), "  minPts: ", minPts, "\n",
      "  clusters (non-noise): ", n_clusters, "\n",
      "  noise points: ", n_noise, "\n",
      "  avg silhouette: ", ifelse(is.na(silhouette_score), "NA", round(silhouette_score, 3)),
      "\n", sep = "")

  plot_idx <- seq_len(nrow(reduced_data))
  pca_vis_plot <- pca_vis[plot_idx, , drop = FALSE]
  labels_plot  <- filtered_labels[plot_idx]

  list(
    data_scaled      = data_scaled,
    keep_cols        = keep_cols,
    pca              = pca,
    reduced_data     = reduced_data,
    pca_vis          = pca_vis,
    best_eps         = best_eps,
    minPts           = minPts,
    labels_raw       = labels,
    labels_filtered  = filtered_labels,
    n_clusters       = n_clusters,
    n_noise          = n_noise,
    silhouette_score = silhouette_score,
    plot_idx         = plot_idx,
    pca_vis_plot     = pca_vis_plot,
    labels_plot      = labels_plot,
    pca_var_plot     = pca_var_plot   
  )
}
```

```{r}
# define function for 3D visualization
visualize_dbscan_3d <- function(
  prep_obj,
  num_pcs_plot = 4,      # PCs 1..num_pcs_plot in 3D combos
  top_n_clusters = 3,    # visualize top-N non-noise clusters
  point_size = 3
) {
  stopifnot(is.list(prep_obj), !is.null(prep_obj$pca), !is.null(prep_obj$labels_filtered))

  pca      <- prep_obj$pca
  labels   <- prep_obj$labels_filtered
  pcscores <- pca$x

  nn <- labels[labels > 0]
  if (length(nn) == 0) stop("No non-noise clusters to visualize.")
  cl_sizes <- sort(table(nn), decreasing = TRUE)
  top_ids  <- as.integer(names(cl_sizes)[seq_len(min(top_n_clusters, length(cl_sizes)))])

  keep <- labels %in% top_ids
  pca_top  <- pcscores[keep, , drop = FALSE]
  labs_top <- factor(labels[keep])

  # palette for exactly top_n_clusters clusters
  cols <- c("#FFFFFF", "#BB021E", "#E7B800")[seq_len(length(unique(labs_top)))]

  num_pcs_plot <- min(num_pcs_plot, ncol(pca_top))
  triples <- combn(num_pcs_plot, 3, simplify = FALSE)

  plots <- vector("list", length(triples))
  names(plots) <- vapply(triples, function(t) sprintf("PC%d_PC%d_PC%d", t[1], t[2], t[3]), character(1))

  for (i in seq_along(triples)) {
    t <- triples[[i]]
    pc1 <- t[1]; pc2 <- t[2]; pc3 <- t[3]

    p <- plot_ly(
      x = ~pca_top[, pc1],
      y = ~pca_top[, pc2],
      z = ~pca_top[, pc3],
      color = ~labs_top,
      colors = cols,
      type = "scatter3d",
      mode  = "markers",
      marker = list(size = point_size)
    ) %>%
      layout(
        title = list(
          text = sprintf("DBSCAN — Top %d Clusters in PCA 3D (PC%d–PC%d–PC%d)",
                         length(unique(labs_top)), pc1, pc2, pc3),
          font = list(color = "white", size = 16)
        ),
        scene = list(
          xaxis = list(title = paste0("PC", pc1), titlefont = list(color = "white"),
                       tickfont = list(color = "white"), backgroundcolor = "#1C1C1C",
                       gridcolor = "gray40", zerolinecolor = "gray40", showbackground = TRUE),
          yaxis = list(title = paste0("PC", pc2), titlefont = list(color = "white"),
                       tickfont = list(color = "white"), backgroundcolor = "#1C1C1C",
                       gridcolor = "gray40", zerolinecolor = "gray40", showbackground = TRUE),
          zaxis = list(title = paste0("PC", pc3), titlefont = list(color = "white"),
                       tickfont = list(color = "white"), backgroundcolor = "#1C1C1C",
                       gridcolor = "gray40", zerolinecolor = "gray40", showbackground = TRUE)
        ),
        legend = list(
          title = list(text = "<b>Cluster</b>", font = list(color = "white")),
          font  = list(color = "white"),
          bgcolor = "#1C1C1C"
        ),
        paper_bgcolor = "#1C1C1C",
        plot_bgcolor  = "#1C1C1C"
      )

    plots[[i]] <- p
  }

  list(
    plots = plots,
    top_clusters = top_ids,
    cluster_sizes = cl_sizes,
    # bubble up diagnostics from function 1 for convenience
    n_clusters = prep_obj$n_clusters,
    n_noise = prep_obj$n_noise,
    silhouette_score = prep_obj$silhouette_score
  )
}
```

```{r}

# define summary function
summarize_clusters <- function(
  prep_obj,
  data,                      # original data frame (must include target)
  target_col = c("ABANDONED_CART", "ABANDONED"),
  min_cluster_size = 100
) {
  stopifnot(is.list(prep_obj), is.data.frame(data))
  target_col <- match.arg(target_col)

  labels <- prep_obj$labels_filtered
  if (length(labels) != nrow(data)) {
    stop("Row count of labels does not match the provided data. Ensure function 1 ran on the same data (no sampling).")
  }
  if (!target_col %in% names(data)) {
    stop(sprintf("Column '%s' not found in `data`.", target_col))
  }

  # print diagnostics from run_pca_dbscan
  cat("\nCluster Diagnostics\n",
      "  clusters (non-noise): ", prep_obj$n_clusters, "\n",
      "  noise points: ", prep_obj$n_noise, "\n",
      "  avg silhouette: ", ifelse(is.na(prep_obj$silhouette_score), "NA", round(prep_obj$silhouette_score, 3)),
      "\n", sep = "")

  library(dplyr); library(ggplot2); library(scales); library(reshape2)

  clust_df <- dplyr::tibble(
    row_id  = seq_len(nrow(data)),
    cluster = as.integer(labels),
    target  = as.integer(data[[target_col]])
  ) %>%
    filter(!is.na(target))

  cluster_rates <- clust_df %>%
    group_by(cluster) %>%
    summarise(
      n = n(),
      abandoned = sum(target == 1),
      rate = abandoned / n,
      .groups = "drop"
    ) %>%
    arrange(desc(n))

  valid_clusters <- cluster_rates %>%
    filter(n > min_cluster_size, cluster != 0) %>%
    pull(cluster)

  gg_rate <- ggplot(cluster_rates %>% filter(cluster %in% valid_clusters),
                    aes(x = factor(cluster), y = rate)) +
    geom_col(fill = "#BB021E") +
    geom_text(aes(label = percent(rate, accuracy = 0.1)),
              vjust = -0.4, color = "white", size = 3.5) +
    labs(
      x = "DBSCAN Cluster",
      y = "Abandonment Rate",
      title = sprintf("Abandonment Rate by DBSCAN Cluster (n > %d) — Target: %s",
                      min_cluster_size, target_col)
    ) +
    ylim(0, 1) +
    theme_minimal(base_size = 13) +
    theme(
      plot.background = element_rect(fill = "#1C1C1C", color = NA),
      panel.background = element_rect(fill = "#1C1C1C", color = NA),
      panel.grid.major.x = element_line(color = "gray30"),
      panel.grid.major.y = element_blank(),
      panel.grid.minor = element_blank(),
      axis.text  = element_text(color = "white", size = 9),
      axis.title = element_text(color = "white"),
      plot.title = element_text(size = 14, face = "bold", color = "white", hjust = 0),
      plot.margin = ggplot2::margin(t = 10, r = 40, b = 10, l = 10)
    )

  # feature means (scaled features from prep_obj$data_scaled)
  data_scaled <- as.data.frame(prep_obj$data_scaled)
  data_scaled$cluster <- labels

  feature_means <- data_scaled %>%
    filter(cluster %in% valid_clusters) %>%
    group_by(cluster) %>%
    summarise(across(where(is.numeric), mean), .groups = "drop")

  cluster_summary <- feature_means %>%
    left_join(cluster_rates %>% dplyr::select(cluster, n, abandoned, abandonment_rate = rate),
              by = "cluster") %>%
    arrange(desc(n))

  cluster_summary_long <- cluster_summary %>%
    dplyr::select(-n, -abandoned) %>%
    melt(id.vars = c("cluster", "abandonment_rate"))

  pal_clusters <- colorRampPalette(c("white", "#BB021E"))(length(unique(cluster_summary_long$cluster)))

  gg_features <- ggplot(cluster_summary_long,
                        aes(x = variable, y = value, fill = factor(cluster))) +
    geom_col(position = "dodge") +
    coord_flip() +
    scale_fill_manual(values = pal_clusters, name = "Cluster") +
    labs(
      x = "Event Feature (scaled)",
      y = "Mean",
      title = sprintf("Event Feature Means by Cluster (n > %d)", min_cluster_size)
    ) +
    theme_minimal(base_size = 13) +
    theme(
      plot.background = element_rect(fill = "#1C1C1C", color = NA),
      panel.background = element_rect(fill = "#1C1C1C", color = NA),
      panel.grid.major.x = element_line(color = "gray30"),
      panel.grid.major.y = element_blank(),
      panel.grid.minor = element_blank(),
      axis.text  = element_text(color = "white", size = 9),
      axis.title = element_text(color = "white"),
      plot.title = element_text(size = 14, face = "bold", color = "white", hjust = 0),
      legend.text  = element_text(color = "white"),
      legend.title = element_text(color = "white"),
      legend.position = "bottom",
      plot.margin = ggplot2::margin(t = 10, r = 40, b = 10, l = 10)
    )

  list(
    cluster_rates     = cluster_rates,
    valid_clusters    = valid_clusters,
    cluster_summary   = cluster_summary,
    rate_plot         = gg_rate,
    feature_plot      = gg_features,
    # surface key diagnostics here too
    n_clusters        = prep_obj$n_clusters,
    n_noise           = prep_obj$n_noise,
    silhouette_score  = prep_obj$silhouette_score
  )
}
```

```{r}
#| fig-height: 10 # figure height has been adjusted to better suit PCA variable plot
#| fig-width: 12
#| 
# Prep + PCA + DBSCAN by calling first function
prep <- run_pca_dbscan(
  data = event_normalized[, -c(1,2)], # remove predictors only here
  n_top_cols = 20, n_pcs = 10, n_contrib_vars = 10
)

# display diagnostics available immediately:
prep$n_clusters
prep$n_noise
prep$silhouette_score

# 3D visualization
viz <- visualize_dbscan_3d(prep, num_pcs_plot = 4, top_n_clusters = 5)
viz$plots[[1]]

# Summaries (pass full data with target column)
summ <- summarize_clusters(prep, data = event_normalized, target_col = "ABANDONED", min_cluster_size = 100)
summ$n_clusters
summ$silhouette_score
summ$rate_plot
summ$feature_plot
```

The DBSCAN model identified three non-noise clusters with an average silhouette score of 0.571, indicating that observations within each cluster are, on average, more similar to one another than to those in other clusters.

The 3D visualization reveals that Cluster 2 passes through the center of Cluster 1; however, the two clusters occupy distinct regions within the feature space. This spatial separation further aligns with the moderate silhouette score, suggesting a meaningful but not absolute distinction between clusters.

Analysis of event feature means highlights behavioral differences across clusters. Specifically, Cluster 2 demonstrates a higher abandonment rate than Cluster 1, with notable disparities in events such as user_engagement and screen_view, which appear to be key drivers of the observed behavioral segmentation. 

### Association Rule mining

By running the same association rule mining process on non-abandoned (purchased) carts, we can distinguish common behaviors (e.g., adding to cart) from those uniquely predictive of abandonment (e.g., frequent cart updates on mobile). Rules unique to the abandoned subset indicate friction or hesitation points, while rules unique to purchased carts suggest smooth conversion paths.

```{r}
### Association Rule Mining on Abandoned Carts

# Filter for abandoned carts only
abandoned_arule <- gao |> filter(ABANDONED == 1)

# Create transactions by CART_ID and event or item behavior
transactions_arule <- abandoned_arule |>
  group_by(CART_ID) |>
  summarise(actions = paste(unique(EVENT_NAME), collapse = ","))

# Convert to 'transactions' object
trans_list_arule <- strsplit(transactions_arule$actions, ",")
txn_arule <- as(trans_list_arule, "transactions")

# Run Apriori algorithm
rules_arule <- apriori(
  txn_arule,
  parameter = list(supp = 0.05, conf = 0.8, minlen = 2, maxlen = 4) # had to tighten parameters because moderate thresholds were producing 1.5mil rules
)

# Inspect and visualize results
inspect(head(sort(rules_arule, by = "lift"), 10))

# Web-like visualization
plot(rules_arule, method = "graph", control = list(type = "items"))

```

The abandoned-cart subset initially generated over 15 million rules, indicating excessive noise and redundancy in low-support event combinations. To improve interpretability, we increased the support and confidence thresholds to retain only the most meaningful behavioral patterns (those occurring in ≥5% of transactions with ≥80% confidence).

The graph above visualizes the strongest co-occurring behavioral events among abandoned carts. Red nodes indicate events with the highest lift, while node size reflects event frequency. Unsurprisingly, `UpdateCart_Cart_Clicked` and `CartPage_Displayed` appear as central, high-lift nodes, suggesting customers frequently revisit or modify their carts without completing a purchase, indicating a key friction point in the checkout process.

```{r purchased arule}
### Association Rule Mining Purchased vs. Abandoned Carts

# Purchased carts subset
purchased_arule <- gao |> filter(ABANDONED == 0)

# Transactions for purchased carts
transactions_purchased <- purchased_arule |>
  group_by(CART_ID) |>
  summarise(actions = paste(unique(EVENT_NAME), collapse = ","))

# Convert to transactions object
trans_list_purchased <- strsplit(transactions_purchased$actions, ",")
txn_purchased <- as(trans_list_purchased, "transactions")

# Generate rules
rules_purchased <- apriori(
  txn_purchased,
  parameter = list(supp = 0.01, conf = 0.6, minlen = 2, maxlen= 4)
)

# Sort and inspect top rules
inspect(head(sort(rules_purchased, by = "lift"), 10))

# Visualize
plot(rules_arule, method = "graph", control = list(type = "items"))
```

Association rule mining was applied to non-abandoned (purchased) carts to uncover the behavioral event patterns most strongly linked to successful checkouts. Using the Apriori algorithm with a support threshold of 1% and a confidence threshold of 60%, we identified frequent co-occurrences of user actions that lead to order completion.

The above network visualization maps these behavioral rules, where node size reflects event frequency (support) and color intensity corresponds to rule strength (lift). Events such as `session_start`, `add_to_cart`, `view_cart`, and `UpdateCart_Cart_Clicked` form the central core of this network, highlighting a consistent, streamlined sequence of engagement leading to purchase.

In contrast to abandoned carts, this structure shows tighter interconnections and higher-lift associations among checkout-related events, suggesting that successful purchasers follow a more linear and predictable path through the ordering funnel. These strong behavioral links indicate efficient task completion and reduced friction points during the checkout process.

In comparison, the abandoned-cart network displayed a more diffuse structure, with weaker and less cohesive relationships between events. High-lift nodes such as `UpdateCart_Cart_Clicked` and `CartPage_Displayed` appeared frequently but were not strongly connected to downstream purchase actions. This pattern suggests that abandoned sessions are characterized by repetitive cart interactions and navigation loops rather than a steady progression toward checkout.

By contrast, the purchased-cart rules indicate a clear and efficient behavioral flow. Users begin with product exploration events like `Categories_PLP_Retrieved` and `view_cart`, then transition smoothly through cart updates and session engagement to order completion. The stronger rule lift and tighter clustering in the purchased segment highlight reduced decision friction and fewer interruptions, suggesting that successful transactions are driven by focused, goal-directed engagement.

```{r arule comparison}
# filter for top 500 rules to shorten processing time into dfs
rules_arule <- head(sort(rules_arule, by = "lift"), 500)
rules_purchased <- head(sort(rules_purchased, by = "lift"), 500)

# Convert to data frames
rules_abandoned_df <- as(rules_arule, "data.frame")
rules_purchased_df <- as(rules_purchased, "data.frame")

# Identify unique or shared antecedents
unique_to_abandoned <- anti_join(rules_abandoned_df, rules_purchased_df, by = "rules")
unique_to_purchased <- anti_join(rules_purchased_df, rules_abandoned_df, by = "rules")
```

```{r visualize comparison}

# Create a common field for grouping
rules_abandoned_df$group <- "Abandoned"
rules_purchased_df$group <- "Purchased"

# Combine into one frame
rules_compare_df <- bind_rows(rules_abandoned_df, rules_purchased_df)

# Limit to top 10 rules
top_rules <- rules_compare_df %>%
  group_by(group) %>%
  arrange(desc(lift)) %>%
  slice_head(n = 10) %>%     # strictly 10 rules per group
  ungroup()


top_rules <- top_rules %>%
  mutate(
    rules_short = gsub("[\\{\\}]", "", rules),   # remove { and }
    rules_short = str_trunc(rules_short, width = 45, side = "right")
  )

# Plot
ggplot(top_rules, aes(x = lift, y = reorder(rules_short, lift), fill = group)) +
  geom_col(alpha = 0.9, show.legend = FALSE) +
  facet_wrap(~ group, scales = "free_y") +
  scale_fill_manual(values = c("Abandoned" = "#BB021E", "Purchased" = "gray70")) +
  labs(
    title = "Top 10 Association Rules by Lift",
    subtitle = "Comparison of behavioral patterns between Abandoned and Purchased Carts",
    x = "Lift",
    y = "Rule (truncated)"
  ) +
  theme_minimal(base_size = 13) +
  theme(
    plot.background  = element_rect(fill = "#1C1C1C", color = NA),
    panel.background = element_rect(fill = "#1C1C1C", color = NA),
    panel.grid.major.x = element_line(color = "gray30"),
    panel.grid.major.y = element_blank(),
    panel.grid.minor   = element_blank(),
    axis.text.x  = element_text(color = "white", size = 9),
    axis.text.y  = element_text(color = "white", size = 8),
    axis.title   = element_text(color = "white"),
    plot.title   = element_text(size = 14, face = "bold", color = "white", hjust = 0),
    plot.subtitle= element_text(size = 11, color = "gray80", hjust = 0),
    strip.text   = element_text(face = "bold", color = "white", size = 11),
    legend.text  = element_text(color = "white"),
    legend.title = element_text(color = "white"),
    legend.position = "bottom",
    plot.margin  = ggplot2::margin(t = 10, r = 40, b = 10, l = 10)
  )
```

By applying the Apriori algorithm to both abandoned and purchased subsets, we were able to compare which event sequences appeared exclusively in abandoned carts versus those leading to successful conversions. Rules unique to abandoned carts—such as frequent cart updates and repeated cart page displays—suggest friction points or hesitation during the checkout process. In contrast, rules prevalent among purchased carts highlight smoother behavioral paths, typically involving fewer cart modifications and more direct navigation to checkout.

This rule-based behavioral segmentation provided an interpretable framework to distinguish between engaged, decisive purchasers and uncertain or distracted browsers, offering actionable insights for interface design improvements, retargeting strategies, and checkout optimization.

```{r}
# Select top N rules
top_abandoned_rules <- head(sort(rules_arule, by = "lift"), 15)
top_purchased_rules <- head(sort(rules_purchased, by = "lift"), 15)

# Combine abandoned + purchased for one unified feature set
all_top_rules <- c(top_abandoned_rules, top_purchased_rules)
rule_strings <- labels(lhs(all_top_rules))

# Collapse all events per cart
cart_patterns <- gao %>%
  group_by(CART_ID) %>%
  summarise(events = paste(unique(EVENT_NAME), collapse = ","))

# Flag whether each rule's antecedent appears in a cart
for (i in seq_along(rule_strings)) {
  terms <- str_remove_all(rule_strings[i], "[\\{\\}]") %>% str_split(",") %>% unlist()
  cart_patterns[[paste0("rule_", i)]] <-
    map_lgl(cart_patterns$events, ~ all(terms %in% str_split(.x, ",")[[1]]))
}

# create single behavioral intensity variable
rule_weights <- quality(all_top_rules)$lift
cart_patterns$RULE_SCORE <- rowSums(cart_patterns[, paste0("rule_", 1:length(all_top_rules))] * rule_weights)

# merge rule features to dataset
gao <- gao |>
  mutate(
    CART_ID = paste(CUSTOMER_ID, CUTOFF_TIMESTAMP, sep = "_")
  )

arule_model_df <- left_join(gao, cart_patterns[, c("CART_ID", "RULE_SCORE")], by = "CART_ID") %>%
  mutate(RULE_SCORE = replace_na(RULE_SCORE, 0))
```

Next, we wanted to see if segmenting the data by business type changed any associative rules. The data was aggregated by `COLD_DRINK_CHANNEL` then mined again.

```{r}
### Association Rule Mining by Cold Drink Channel

# Choose a channel subset (e.g., Restaurant, Workplace, etc.) 
channels <- unique(gao$COLD_DRINK_CHANNEL_DESCRIPTION)
print(channels)

# Function to safely run Apriori per channel
run_arules_by_channel <- function(df, channel, supp = 0.03, conf = 0.7, minlen = 2, maxlen = 4) {
  message(sprintf("\n🧃 Running Apriori for channel: %s", channel))
  
  # Filter abandoned carts for that channel
  sub_df <- df %>%
    filter(ABANDONED == 1,
           COLD_DRINK_CHANNEL_DESCRIPTION == channel) %>%
    group_by(CART_ID) %>%
    summarise(actions = paste(unique(EVENT_NAME), collapse = ",")) %>%
    ungroup()
  
  # Skip if too few carts
  if (nrow(sub_df) < 50) {
    warning(paste("Skipped channel", channel, "— too few transactions"))
    return(NULL)
  }
  
  # Convert to transactions
  txn_list <- strsplit(sub_df$actions, ",")
  txn <- as(txn_list, "transactions")
  
  # Run Apriori
  rules <- apriori(
    txn,
    parameter = list(supp = supp, conf = conf, minlen = minlen, maxlen = maxlen)
  )
  
  # Return top 15 by lift
  head(sort(rules, by = "lift"), 15)
}

# --- 2. Run for each channel and store results ---
channel_rules <- lapply(channels, function(ch) run_arules_by_channel(gao, ch))
names(channel_rules) <- channels

# --- 3. Inspect one example (e.g., "Restaurant") ---
inspect(channel_rules[["Restaurant"]])

# --- 4. Optional: visualize rules for each segment ---
plot(channel_rules[["Restaurant"]], method = "graph",
     main = "Restaurant Channel – Abandoned Cart Rules")

plot(channel_rules[["Workplace"]], method = "grouped",
     main = "Workplace Channel – Rule Clusters")
```

```{r}
# Rank segments by number of abandoned carts
abandon_counts <- gao %>%
  filter(ABANDONED == 1) %>%
  dplyr::count(COLD_DRINK_CHANNEL_DESCRIPTION, sort = TRUE)

top2 <- head(abandon_counts$COLD_DRINK_CHANNEL_DESCRIPTION, 2)
bottom2 <- tail(abandon_counts$COLD_DRINK_CHANNEL_DESCRIPTION, 2)

selected_channels <- c(top2, bottom2)
print(abandon_counts)
cat("\nSelected channels (top 2 + bottom 2 by abandoned carts):\n")
print(selected_channels)

# Define a function to run Apriori analysis for one channel
run_arules_by_channel <- function(df, channel,
                                  supp = 0.03, conf = 0.7,
                                  minlen = 2, maxlen = 4) {
  message(sprintf("\nRunning Apriori for channel: %s", channel))
  
  sub_df <- df %>%
    filter(ABANDONED == 1,
           COLD_DRINK_CHANNEL_DESCRIPTION == channel) %>%
    group_by(CART_ID) %>%
    summarise(actions = paste(unique(EVENT_NAME), collapse = ","), .groups = "drop")
  
  # Skip channel if there are too few carts or no valid events
  if (nrow(sub_df) < 50) {
    message(sprintf("Skipped %s — only %d carts", channel, nrow(sub_df)))
    return(NULL)
  }
  if (all(is.na(sub_df$actions)) || all(sub_df$actions == "")) {
    message(sprintf("Skipped %s — no valid events", channel))
    return(NULL)
  }
  
  # Clean and validate the transaction list
  txn_list <- strsplit(sub_df$actions, ",")
  txn_list <- txn_list[lengths(txn_list) > 0 & !sapply(txn_list, function(x) all(is.na(x) | x == ""))]
  txn_list <- lapply(txn_list, function(x) as.character(na.omit(trimws(x))))
  txn_list <- txn_list[sapply(txn_list, function(x) length(x) > 0)]
  
  # Check that the transaction list is valid before coercion
  if (length(txn_list) == 0 || any(!sapply(txn_list, is.character))) {
    message(sprintf("Skipped %s — invalid transaction list structure", channel))
    return(NULL)
  }
  
  # Convert the list to a transactions object
  txn <- tryCatch({
    as(txn_list, "transactions")
  }, error = function(e) {
    message(sprintf("Conversion failed for %s: %s", channel, e$message))
    return(NULL)
  })
  
  if (is.null(txn)) {
    message(sprintf("No valid transactions for %s", channel))
    return(NULL)
  }
  
  # Run Apriori algorithm on the transaction set
  rules <- tryCatch({
    apriori(txn, parameter = list(supp = supp, conf = conf,
                                  minlen = minlen, maxlen = maxlen))
  }, error = function(e) {
    message(sprintf("Apriori failed for %s: %s", channel, e$message))
    return(NULL)
  })
  
  if (is.null(rules) || length(rules) == 0) {
    message(sprintf("No rules generated for %s", channel))
    return(NULL)
  }
  
  # Return the top 15 rules by lift
  head(sort(rules, by = "lift"), 15)
}

# Run Apriori for each selected channel (top 2 and bottom 2 segments)
channel_rules <- list()

for (ch in selected_channels) {
  cat("\nRunning Apriori for:", ch, "\n")
  res <- tryCatch(
    run_arules_by_channel(gao, ch, minlen = 1),
    error = function(e) {
      cat("Error in", ch, ":", e$message, "\n")
      NULL
    }
  )
  channel_rules[[ch]] <- res
}

names(channel_rules) <- selected_channels

# Remove NULL results before combining data
channel_rules <- channel_rules[!sapply(channel_rules, is.null)]
selected_channels <- names(channel_rules)

# Combine results from all channels into a single data frame
rules_df <- map_dfr(selected_channels, function(ch) {
  rset <- channel_rules[[ch]] 
  as(rset, "data.frame") %>%
    mutate(COLD_DRINK_CHANNEL_DESCRIPTION = ch)
})

# Extract antecedent and consequent from rule strings
rules_df <- rules_df %>%
  mutate(
    antecedent = gsub("\\{(.*)\\}.*", "\\1", rules),
    consequent = gsub(".*\\{(.*)\\}", "\\1", rules)
  )

# Compare lift variation across segments for the same rule
lift_diff <- rules_df %>%
  group_by(antecedent, consequent) %>%
  summarise(
    lift_range = max(lift, na.rm = TRUE) - min(lift, na.rm = TRUE),
    top_channel = COLD_DRINK_CHANNEL_DESCRIPTION[which.max(lift)],
    bottom_channel = COLD_DRINK_CHANNEL_DESCRIPTION[which.min(lift)],
    .groups = "drop"
  ) %>%
  arrange(desc(lift_range)) %>%
  slice_head(n = 10)

print(lift_diff)
```

```{r}
rules_df$COLD_DRINK_CHANNEL_DESCRIPTION <- factor(
  rules_df$COLD_DRINK_CHANNEL_DESCRIPTION,
  levels = c("Restaurant", "Distributor", "Hot Beverage", "Clinic")
)

# Visualize rule lift by segment
# Filter for top 2 segments
rules_top2 <- rules_df %>%
  filter(COLD_DRINK_CHANNEL_DESCRIPTION %in% c("Restaurant", "Distributor"))
 rules_top2
ggplot(rules_top2,
       aes(x = COLD_DRINK_CHANNEL_DESCRIPTION,
           y = lift,
           color = COLD_DRINK_CHANNEL_DESCRIPTION)) +
  geom_jitter(width = 0.15, size = 2, alpha = 0.8) +
  facet_wrap(~ consequent, scales = "free_y", ncol = 3) +
  scale_color_manual(values = c(
    "Restaurant"  = "#BB021E",   # Swire red
    "Distributor" = "gray70"
  )) +
  labs(
    title = "Association Rule Lift – Top 2 Segments (Restaurant & Distributor)",
    subtitle = "Higher lift indicates stronger, recurring behavioral patterns in high-volume channels",
    x = "Cold Drink Channel Segment",
    y = "Lift"
  ) +
  theme_minimal(base_size = 13) +
  theme(
    plot.background  = element_rect(fill = "#1C1C1C", color = NA),
    panel.background = element_rect(fill = "#1C1C1C", color = NA),
    panel.grid.major.x = element_blank(),
    panel.grid.major.y = element_line(color = "gray30"),
    panel.grid.minor   = element_blank(),
    axis.text.x  = element_text(color = "white", size = 9, angle = 25, hjust = 1),
    axis.text.y  = element_text(color = "white", size = 9),
    axis.title   = element_text(color = "white"),
    plot.title   = element_text(size = 14, face = "bold", color = "white", hjust = 0),
    plot.subtitle= element_text(size = 11, color = "gray80", hjust = 0),
    strip.text   = element_text(face = "bold", color = "white", size = 11),
    legend.position = "top",
    legend.title = element_blank(),
    legend.text  = element_text(color = "white"),
    plot.margin  = ggplot2::margin(t = 10, r = 40, b = 10, l = 10)
  )

# Filter for bottom 2 segments
rules_bottom2 <- rules_df %>%
  filter(COLD_DRINK_CHANNEL_DESCRIPTION %in% c("Hot Beverage", "Clinic"))

rules_bottom2

ggplot(rules_bottom2,
       aes(x = COLD_DRINK_CHANNEL_DESCRIPTION,
           y = lift,
           color = COLD_DRINK_CHANNEL_DESCRIPTION)) +
  geom_jitter(width = 0.15, size = 2, alpha = 0.8) +
  facet_wrap(~ consequent, scales = "free_y", ncol = 3) +
  scale_color_manual(values = c(
    "Hot Beverage" = "gray70",
    "Clinic"       = "white"
  )) +
  labs(
    title = "Association Rule Lift – Bottom 2 Segments (Hot Beverage & Clinic)",
    subtitle = "Lower lift values indicate weaker or less consistent behavioral co-occurrence patterns",
    x = "Cold Drink Channel Segment",
    y = "Lift"
  ) +
  theme_minimal(base_size = 13) +
  theme(
    plot.background  = element_rect(fill = "#1C1C1C", color = NA),
    panel.background = element_rect(fill = "#1C1C1C", color = NA),
    panel.grid.major.x = element_blank(),
    panel.grid.major.y = element_line(color = "gray30"),
    panel.grid.minor   = element_blank(),
    axis.text.x  = element_text(color = "white", size = 9, angle = 25, hjust = 1),
    axis.text.y  = element_text(color = "white", size = 9),
    axis.title   = element_text(color = "white"),
    plot.title   = element_text(size = 14, face = "bold", color = "white", hjust = 0),
    plot.subtitle= element_text(size = 11, color = "gray80", hjust = 0),
    strip.text   = element_text(face = "bold", color = "white", size = 11),
    legend.position = "top",
    legend.title = element_blank(),
    legend.text  = element_text(color = "white"),
    plot.margin  = ggplot2::margin(t = 10, r = 40, b = 10, l = 10)
  )
```

Large commercial accounts (particularly Restaurants and Distributors) show the strongest and most repetitive behavioral patterns in the MyCoke360 platform, yet also face the highest cart abandonment rates. Their patterns indicate it is likely structural and process-related friction rather than lack of intent. These users often know what they want to buy but are slowed down by workflow complexity and digital interface limitations.

In contrast, the Hot Beverage and Clinic segments exhibit weaker and more diffuse association rule lift patterns, reflecting less structured and less frequent digital ordering behavior. These smaller customers are not disengaged, but their interactions on the MyCoke360 platform are sporadic, simple, and low-repetition, suggesting occasional rather than habitual use. Their behavioral data points to transactional simplicity rather than workflow friction. 

Together, these visualizations highlight that abandonment risk and behavioral predictability are both driven by customer scale and interaction complexity: large accounts behave consistently but face friction points that drive abandonment, while smaller accounts act more sporadically but with lower overall risk.

### Logistic Regression Model

Originally, our baseline model achieved an AUC of approximately 0.85, meaning it correctly distinguished between abandoned and completed carts about 85% of the time. This seemed high for a basic model, since we have an 85/15 split on the majority/minority class, it was predicting the majority class the entire time.

Moving forward, we pared down the analysis variables and fixed the data leakage that we saw in the first model. This approach allows us to move beyond simple one-to-one relationships and identify combinations of behaviors that meaningfully change the odds of abandonment.

```{r}
# Check for numeric variables in current dataset
gao |>
  glimpse()

# New model dataframe
lr <- gao |>
  filter(!EVENT_NAME %in% c('add_to_cart', 'update_cart', 'UpdateCart_Cart_Clicked')) |>
  dplyr::select(-c(WINDOW_ID, ITEMS, ANCHOR_DATE, SALES_OFFICE_DESC, DISTRIBUTION_MODE_DESC, CUTOFF_TIMESTAMP, CUSTOMER_ID, CART_ID, EVENT_PAGE_NAME, EVENT_PAGE_TITLE, CREATED_DATE_ADJUSTED_MATCH, EVENT_TIMESTAMP_ADJ, CUSTOMER_SUB_TRADE_CHANNEL_DESCRIPTION, TOTAL_ITEMS, PURCHASE_EVENT, DAYS_SINCE_LAST_PURCHASE, EVENT_NAME)) |>
  mutate(
    # convert all character columns to factors
    across(where(is.character), as.factor)
  )

# Check that mutation and variable removal worked
lr |>
  glimpse()

lr |>
  str()
```

```{r}
set.seed(673)
idx <- createDataPartition(lr$ABANDONED, p = 0.7, list = FALSE)
train <- lr[idx, ]
test  <- lr[-idx, ]

# Fit logistic regression
lr_fit <- speedglm(ABANDONED ~ ., data = train, family = binomial())
summary(lr_fit)

# Evaluate on holdout
test$pred_prob <- predict(lr_fit, newdata = test, type = "response")
roc_obj <- roc(response = test$ABANDONED, predictor = test$pred_prob)
auc_val <- auc(roc_obj)

test$pred_class <- ifelse(test$pred_prob >= 0.5, 1, 0)

cat("AUC:", round(auc_val, 3), "\n")
caret::confusionMatrix(
factor(test$pred_class, levels = c(0,1)),
factor(test$ABANDONED, levels = c(0,1)),
positive = "1"
)
```

This logistic regression indicates that there is a statistically significant positive relationship between use of mobile device (as opposed to a tablet) and cart abandonment. Additionally, using a ChromeOS increases likelihood of abandonment while using an iOS or Linux OS decreases likelihood of abandonment. The frequency was also statistically significant. While a frequency of every 3 weeks had a positive relationship with abandonment, a frequency of every week or every 4 weeks was associated with lower abandonment. Certain anchor days of the week also had differing relationships with cart abandonment. Tuesday was most correlated with abandonment with Monday, Wednesday, and Thursday following behind. Interestingly, Sunday's correlation was negative and Saturday's was not statistically significant. Certain sales offices also had positive or negative correlations with cart abandonment. The highest correlations were with offices G151 and G152 located in Scottsbluff, NE and Cheyenne, WY respectively. Finally, the BK/OF/SL (bulk distribution, ofs, sideload) combination of distribution modes had the highest statistically significant positive correlation with cart abandonment. Most cutoff times had a negative correlation with cart abandonment. The cutoff times with the strongest negative correlation (>2) were 10am and 2:30pm. The type of customer (`COLD_DRINK_CHANNEL_DESCRIPTION`) of 'Hot Beverage' was highly negatively correlated with cart abandonment. Finally, the `ITEM_BIN` and `DEVICE_MOBILE_BRAND_NAME` variables had mixed results - mostly not statistically significant. These results provide valuable insights into cart abandonment that can be further explored.

#### Device and OS

```{r}
# Plot abandonment rate by device
lr1 <- gao |>
  group_by(DEVICE_CATEGORY) |>
  summarise(
    abandonment_rate = mean(ABANDONED, na.rm = TRUE),
    n = n()
  ) |>
  ggplot(aes(x = DEVICE_CATEGORY, y = abandonment_rate, fill = DEVICE_CATEGORY == 'mobile')) +
  geom_col(alpha = 0.8) +  # Swire Coca-Cola red
  scale_fill_manual(values = c("TRUE" = "#BB021E", "FALSE" = "gray70")) +
  labs(
    title = "Abandonment Rate by Device",
    x = "Device Type",
    y = "Abandonment Rate"
  ) +
  theme_minimal() +
  theme(
    axis.text.x = element_text(hjust = 0.5),
    plot.title = element_text(color = "black"),
    axis.title = element_text(color = "black"),
    axis.text = element_text(color = "black"),
    legend.position = "none"
  )

# Extract logistic regression coefficients
coef_df <- data.frame(
  term = names(coef(lr_fit)),
  estimate = coef(lr_fit)
) |>
  filter(grepl("DEVICE_OPERATING_SYSTEM", term)) |>
  mutate(
    OS = gsub("DEVICE_OPERATING_SYSTEM", "", term)   # clean term name
  ) |>
  filter(!OS %in% c("Macintosh", "Windows")) |>      # remove unwanted OS
  mutate(
    color = case_when(
      OS == "Chrome OS" ~ "#BB021E",                 # red
      OS %in% c("iOS", "Linux") ~ "black",
      TRUE ~ "gray70"
    )
  )

# Plot
lr2 <- ggplot(coef_df, aes(x = reorder(OS, estimate), y = estimate, fill = color)) +
  geom_col() +
  scale_fill_identity() +
  labs(
    title = "Effect of OS on Abandonment",
    x = "Operating System",
    y = "Logistic Regression Coefficient"
  ) +
  theme_minimal() +
  theme(
    axis.text.x = element_text(hjust = 0.5),
    plot.title = element_text(color = "black"),
    axis.title = element_text(color = "black"),
    axis.text = element_text(color = "black"),
    legend.position = "none"
  )

lr1 | lr2
```

The plot on the left combined with the logistic regression clearly shows that mobile devices have a statistically significant higher level of cart abandonment. This may be because of difficulty placing orders on a smaller mobile screen or may be due to issues with the mobile application. It may be possible to decrease cart abandonment by encouraging or incentivising customers to use tablets or desktop application.

The plot on the right shows the statistically significant OS effects from the logistic regression. It is clear that Linux and iOS have a negative correlation with cart abandonment while Chrome OS has a positive correlation. It would be difficult to control customer OS but this gives us valuable information that could be used to target customers using a Chrome OS to help decrease their abandonment rates. 

#### Order Frequency and Anchor Day of Week

```{r}
# Extract coefficients from your logistic regression model
freq_coef_df <- data.frame(
  term = names(coef(lr_fit)),
  estimate = coef(lr_fit)
) |>
  # Keep only frequency terms
  filter(grepl("FREQUENCY_FINAL", term)) |>
  mutate(
    # Clean term names
    frequency = gsub("FREQUENCY_FINAL", "", term),
    # Assign colors
    fill_color = case_when(
      frequency == "Every 3 Weeks" ~ "#BB021E",          # red
      frequency %in% c("Every Week", "Every 4 Weeks") ~ "black",
      TRUE ~ "gray70"
    )
  )

# Plot coefficients
lr3 <- ggplot(freq_coef_df, aes(x = reorder(frequency, estimate), y = estimate, fill = fill_color)) +
  geom_col() +
  scale_fill_identity() +
  labs(
    title = "Effect of Frequency on Cart Abandonment",
    x = "Frequency",
    y = "Logistic Regression Coefficient"
  ) +
  theme_minimal() +
  theme(
    axis.text.x = element_text(hjust = 0.5, color = "black"),
    axis.text.y = element_text(color = "black"),
    axis.title = element_text(color = "black"),
    plot.title = element_text(color = "#BB021E", hjust = 0),
    legend.position = "none"
  )

# Plot ANCHOR_DAY_OF_WEEK
important_days <- c("Tuesday", "Monday", "Wednesday", "Thursday")

day_levels <- c("Sunday", "Monday", "Tuesday", "Wednesday", "Thursday", "Friday", "Saturday")

lr4 <- gao |>
  group_by(ANCHOR_DAY_OF_WEEK) |>
  summarise(abandonment_rate = mean(ABANDONED, na.rm = TRUE)) |>
  mutate(
    # Order days correctly
    ANCHOR_DAY_OF_WEEK = factor(ANCHOR_DAY_OF_WEEK, levels = day_levels),
    # Highlight important days
    fill_color = ifelse(ANCHOR_DAY_OF_WEEK %in% important_days, "#BB021E", "gray70")
  ) |>
  ggplot(aes(x = ANCHOR_DAY_OF_WEEK, y = abandonment_rate, fill = fill_color)) +
  geom_col() +
  scale_fill_identity() +
  labs(
    title = "Abandonment by Anchor Day",
    x = "Day of Week",
    y = "Abandonment Rate"
  ) +
  theme_minimal() +
  theme(
    axis.text.x = element_text(angle = 45, hjust = 0.5, color = "black"),
    axis.text.y = element_text(color = "black"),
    axis.title = element_text(color = "black"),
    plot.title = element_text(color = "#BB021E", hjust = 0),
    legend.position = "none"
  )

lr3 | lr4
```

These plots clearly show that ordering every week or every 4 weeks is associated with less cart abandonment while ordering every 3 weeks is correlated with more cart abandonment. Additionally, the anchor weekdays Monday - Thursday have the highest abandonment rates. This helps use understand who Swire should target with their interventions to reduce cart abandonment. 

#### Sales Office and Distribution Mode

```{r}
# Sales office plot
important_offices <- c("G151", "G152")

sales_office_plot <- gao |>
  group_by(SALES_OFFICE) |>
  summarise(abandonment_rate = mean(ABANDONED, na.rm = TRUE)) |>
  ungroup() |>
  slice_max(abandonment_rate, n = 10) |>   # keep top 10
  mutate(
    fill_color = ifelse(SALES_OFFICE %in% important_offices, "#BB021E", "gray70")
  ) |>
  ggplot(aes(x = reorder(SALES_OFFICE, abandonment_rate), y = abandonment_rate, fill = fill_color)) +
  geom_col() +
  scale_fill_identity() +
  labs(
    title = "Abandonment by Sales Office",
    subtitle = "Top 10",
    x = "Sales Office",
    y = "Abandonment Rate"
  ) +
  theme_minimal() +
  theme(
    axis.text.x = element_text(angle = 45, hjust = 0.5, color = "black"),
    axis.text.y = element_text(color = "black"),
    axis.title = element_text(color = "black"),
    plot.title = element_text(color = "#BB021E", hjust = 0),
    legend.position = "none"
  )

# Distribution mode plot
important_distribution <- "BK; OF; SL"

distribution_plot <- gao |>
  group_by(DISTRIBUTION_MODE) |>
  summarise(abandonment_rate = mean(ABANDONED, na.rm = TRUE)) |>
  ungroup() |>
  slice_max(abandonment_rate, n = 10) |>   # keep top 10
  mutate(
    fill_color = ifelse(DISTRIBUTION_MODE == important_distribution, "#BB021E", "gray70")
  ) |>
  ggplot(aes(x = reorder(DISTRIBUTION_MODE, abandonment_rate), y = abandonment_rate, fill = fill_color)) +
  geom_col() +
  scale_fill_identity() +
  labs(
    title = "Abandonment by Distribution Mode",
    subtitle = "Top 10",
    x = "Distribution Mode",
    y = "Abandonment Rate"
  ) +
  theme_minimal() +
  theme(
    axis.text.x = element_text(angle = 45, hjust = 0.5, color = "black"),
    axis.text.y = element_text(color = "black"),
    axis.title = element_text(color = "black"),
    plot.title = element_text(color = "#BB021E", hjust = 0),
    legend.position = "none"
  )

# Facet wrap plots
sales_office_plot | distribution_plot
```

These two plots show the top 10 sales offices and distribution modes by abandonment rate with high statistically significant correlation with abandonment highlighted in red. Offices G151 and G152 located in Scottsbluff, NE and Cheyenne, WY respectively had a high correlation with cart abandonment when running the logistic model. As such, it may be that businesses ordering from Cheyenne and Scottsbluff are more likely to abandon their carts because of the rural nature of the distributor offices. There may be issues especially with the Cheyenne location that cause customers to abandon their carts. This should be further studied to understand what the issue could be. Also, it seems that the combination of bulk distribution, ofs, and sideload lead to increased abandonment. This could be because of the variety of ways in which the goods are delivered. This might complicate the ordering process which leads to increased cart abandonment. Further industry knowledge is necessary to definitively understand the significance of these factors in cart abandonment. 

## Summary of Findings

### Prevention of Target Variable Leakage

In our original modeling efforts, we realized that the `EVENT_NAME` variable we used to create the target variable was resulting in target leakage into our models. This was evident because the `EVENT_NAME` variable was by far the highest feature importance in our Random Forest model. Additionally, when analyzing cart abandonment rates by `EVENT_NAME`, we found that the `EVENT_NAME` values that were used to create the target abandonment variable were the only ones with an abandonment rate > 1. As a result, in our final models, we not only removed the target but also filtered to exclude the event names that we used to build our cart abandonment variable. This ensured that there was no target variable leakage. One limitation of our efforts is that because we removed rows from our data to prevent target variable leakage, we may not be capturing the full behavioral panel that leads to cart abandonment. However, we are confident that our analysis captures the most significant behaviors leading to cart abandonment.

### Acknowledgement of Limitations

During our analysis, we encountered several limitations that constrained our modeling and interpretability:

 - **Lack of control data:** No pre-treatment window to analyze whether the platform changes casued an effect in cart abandonment.
 - **Missing data:** `google_analytics` doesn't capture all events - it appears that some were deleted or not logged properly.
 - **Outliers:** Known outliers in the data that skewed results.
 - **Highly correlated features:** Many features exhibited multicollinearity, complicating coefficient interpretation in regression models. It also caused increased difficulty in separating observations for clustering.

### Conclusion of Findings

Our final modeling efforts identified clear behavioral and contextual patterns influencing digital cart abandonment on the MyCoke360 platform. Unfortunately, during compilation we suffered some data leaking in our logistic regression models that we haven't quite rooted out yet. However, after addressing early leakage issues, our logistic regression models could be capable of providing interpretable, credible insights into how user interactions relate to purchase completion. We will continue to address leakage issues this week.

Key findings include:

 - **Device category played a significant role:** mobile users showed a higher probability of abandonment compared to desktop users, consistent with potential usability friction on smaller screens.

 - **Cart size and item volume were strong predictors:** very small and very large carts were more likely to be abandoned, suggesting both low-intent “browsers” and overwhelmed bulk purchasers may disengage before checkout.

 - **Days since last purchase negatively correlated with abandonment:** customers who purchased more recently were less likely to abandon their carts, indicating that engagement and familiarity reduce drop-off risk.

 - **Event-level analysis:** revealed that repetitive behaviors such as `UpdateCart_Cart_Clicked` and frequent page revisits are associated with hesitation and increased abandonment odds.

After preventing target leakage and ensuring cart-level independence between training and testing data, the final logistic regression achieved a realistic AUC between 0.70–0.75, indicating moderate predictive ability while maintaining interpretability.

These results reinforce the importance of interface simplification, proactive cart reminders, and behavioral segmentation as potential interventions. While the limited time window of available data constrained causal inference and time-series modeling, this framework provides a validated foundation for expanding predictive monitoring and designing future experiments that can directly quantify improvements in conversion and digital engagement.

### Future Continuation 

While our models successfully described behaviors associated with cart abandonment, there are several opportunities available for further examination and refinement. 

Future  continuation upon this modeling could include:

 - Causal Inference: Expanding dataset to include pre-treatment data. Introduce experimental designs to estimate possible causal impact of the new platform. 
 
 - Deployment and monitoring: Package the final model into a production-ready dashboard or API, enabling continuous monitoring of predicted abandonment risk and real-time alerts for high-risk carts.
 
 - Temporal and sequence modeling: Also requiring an expanded dataset that includes pre-treatment data, we could implement time-aware models to capture event-level sequences and transitions that precede abandonment, allowing us to better understand behavioral activity through the ordering funnel.

## Group Member Contributions

Section | Contributor(s)
--- | ---
Introduction | Eliza
Feature Engineering- Cart ID | Eliza
Feature Engineering- Target Variable | Eliza
Feature Engineering- Days Since Last Purchased | Ashley
Feature Engineering- Number of Items | Eliza
Feature Engineering- Order Window ID | Kyle
Items in Abandoned Carts | Kyle
Events and Abandoned Carts | Kyle
Time-Series Analysis | Eliza
Random Forest | Eliza
Hierarchical and GMM Clustering | Ashley
DBSCAN Clustering | Kyle
Association Rule Mining | Ashley
Basic LM | Ashley
Updated LM | Eliza / Ashley
Summary of Findings | Eliza / Ashley
Compilation | Eliza / Ashley / Lindsey
Editing | All